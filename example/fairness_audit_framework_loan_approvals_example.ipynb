{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a68a4f58",
   "metadata": {},
   "source": [
    "# Loan Approval Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9055ff7",
   "metadata": {},
   "source": [
    "## Step 1: Historical Context Assessment\n",
    "\n",
    "### Domain: Consumer lending / loan approvals\n",
    "\n",
    "Loan approval systems are critical decision tools that affect access to credit, housing, and economic opportunity. Historical evidence shows that marginalized communities — especially by race, gender, and geography — have faced significant discrimination in lending practices.\n",
    "\n",
    "### Documented patterns of harm\n",
    "\n",
    "| Historical pattern                         | Source                                | Relevance to system                          |\n",
    "|-------------------------------------------|---------------------------------------|----------------------------------------------|\n",
    "| Redlining of minority neighborhoods       | U.S. HUD, CFPB Reports                | Loan access often tied to ZIP codes or geography |\n",
    "| Gender bias in financial credit decisions | CFPB, Federal Reserve studies         | Women historically receive smaller loans, more denials |\n",
    "| Algorithmic bias in credit risk models    | Research (e.g., Hurley & Adebayo, 2016) | ML systems may learn from biased historical labels |\n",
    "| Lack of representation in training data   | Data & Society, AI Now Institute      | Underrepresentation can distort learned patterns |\n",
    "\n",
    "### Implications\n",
    "\n",
    "- Features such as **income**, **employment type**, or **property area** may act as proxies for protected attributes.\n",
    "- Labels (i.e., past loan approvals) may reflect **systemic or subjective discrimination**.\n",
    "- Model outcomes should be evaluated for fairness across **gender** and **region**, and **intersectional subgroups** (e.g., Female + Rural).\n",
    "\n",
    "### Outputs to feed into:\n",
    "- Fairness definition selection\n",
    "- Bias source identification\n",
    "- Metric prioritization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2bfea7",
   "metadata": {},
   "source": [
    "## Step 2: Fairness Definition Selection\n",
    "\n",
    "### System impact\n",
    "\n",
    "This loan approval system determines access to financial credit. The **harm** in this context is:\n",
    "- Denial of opportunity to qualified applicants\n",
    "- Reinforcement of historical disparities in credit access\n",
    "\n",
    "Therefore, our fairness priorities focus on ensuring **equal access for equally qualified individuals** and **equity in approval rates**.\n",
    "\n",
    "---\n",
    "\n",
    "### Selected fairness definitions\n",
    "\n",
    "| Definition             | Why it was selected                                  |\n",
    "|------------------------|------------------------------------------------------|\n",
    "| **Equal Opportunity**  | Ensures **true positive rate (TPR)** is similar across groups → qualified individuals are not unfairly denied. |\n",
    "| **Demographic Parity** | Tracks whether **approval rates** reflect population fairness goals, even if not optimized for parity. |\n",
    "\n",
    "These definitions were chosen to balance:\n",
    "- **Legal defensibility** (Equal Opportunity relates to disparate impact standards)\n",
    "- **Public fairness expectations** (representation fairness for approval outcomes)\n",
    "\n",
    "---\n",
    "\n",
    "### Application plan\n",
    "\n",
    "- Use **True Positive Rate Difference** to measure Equal Opportunity.\n",
    "- Use **Positive Prediction Rate Difference** for Demographic Parity.\n",
    "- Where needed, extend to **intersectional subgroups** (e.g., Gender + Property Area).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a9f96",
   "metadata": {},
   "source": [
    "## Step 3: Bias Source Identification\n",
    "\n",
    "This section outlines where bias may enter the ML pipeline for loan approvals, from data collection through deployment.\n",
    "\n",
    "\n",
    "### Pipeline bias map\n",
    "\n",
    "| Stage               | Bias type             | Example for this project                                  | Risk? | Mitigation Idea |\n",
    "|---------------------|------------------------|-----------------------------------------------------------|-------|------------------|\n",
    "| Data Collection      | Representation Bias    | Under-sampling of rural or female applicants              | Medium | Stratified sampling or oversampling |\n",
    "| Labeling             | Historical Bias        | Past approvals reflect biased human decisions             | High   | Reweight labels or debias targets |\n",
    "| Feature Engineering  | Measurement Bias       | Income may encode systemic disparities                    | High   | Examine correlation with protected attributes |\n",
    "| Model Optimization   | Optimization Bias      | Trained purely for accuracy, ignoring fairness constraints| Medium | Consider fair-aware objective or thresholds |\n",
    "| Evaluation           | Aggregation Bias       | Overall accuracy may hide subgroup disparities            | High   | Always report per-group metrics |\n",
    "| Deployment           | Feedback Loops         | Approved users influence future data                      | Low    | Monitor deployment skew |\n",
    "\n",
    "---\n",
    "\n",
    "### Next actions\n",
    "\n",
    "- Analyze distribution of **Gender**, **Property_Area**, **LoanAmount**, etc.\n",
    "- Check correlations between features and protected attributes\n",
    "- Flag features that may act as **proxies for race, gender, or geography**\n",
    "- Document all risks in final report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e0ab435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 598 entries, 0 to 597\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Loan_ID            598 non-null    object \n",
      " 1   Gender             598 non-null    object \n",
      " 2   Married            598 non-null    object \n",
      " 3   Dependents         586 non-null    float64\n",
      " 4   Education          598 non-null    object \n",
      " 5   Self_Employed      598 non-null    object \n",
      " 6   ApplicantIncome    598 non-null    int64  \n",
      " 7   CoapplicantIncome  598 non-null    float64\n",
      " 8   LoanAmount         577 non-null    float64\n",
      " 9   Loan_Amount_Term   584 non-null    float64\n",
      " 10  Credit_History     549 non-null    float64\n",
      " 11  Property_Area      598 non-null    object \n",
      " 12  Loan_Status        598 non-null    object \n",
      "dtypes: float64(5), int64(1), object(7)\n",
      "memory usage: 60.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001006</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001008</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married  Dependents     Education Self_Employed  \\\n",
       "0  LP001002   Male      No         0.0      Graduate            No   \n",
       "1  LP001003   Male     Yes         1.0      Graduate            No   \n",
       "2  LP001005   Male     Yes         0.0      Graduate           Yes   \n",
       "3  LP001006   Male     Yes         0.0  Not Graduate            No   \n",
       "4  LP001008   Male      No         0.0      Graduate            No   \n",
       "\n",
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5849                0.0         NaN             360.0   \n",
       "1             4583             1508.0       128.0             360.0   \n",
       "2             3000                0.0        66.0             360.0   \n",
       "3             2583             2358.0       120.0             360.0   \n",
       "4             6000                0.0       141.0             360.0   \n",
       "\n",
       "   Credit_History Property_Area Loan_Status  \n",
       "0             1.0         Urban           Y  \n",
       "1             1.0         Rural           N  \n",
       "2             1.0         Urban           Y  \n",
       "3             1.0         Urban           Y  \n",
       "4             1.0         Urban           Y  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and inspect the dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"fairness_audit_framework/example/LoanApprovalPrediction.csv\")\n",
    "\n",
    "# Display basic information\n",
    "df.info()\n",
    "\n",
    "# Preview the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "982175b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan_Status unique values: ['Y' 'N']\n",
      "Gender unique values: ['Male' 'Female']\n",
      "Property_Area unique values: ['Urban' 'Rural' 'Semiurban']\n"
     ]
    }
   ],
   "source": [
    "print(\"Loan_Status unique values:\", df[\"Loan_Status\"].unique())\n",
    "print(\"Gender unique values:\", df[\"Gender\"].unique())\n",
    "print(\"Property_Area unique values:\", df[\"Property_Area\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f43b9135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approval Rate by Gender:\n",
      "Gender\n",
      "0    0.666667\n",
      "1    0.691992\n",
      "Name: Loan_Status, dtype: float64\n",
      "\n",
      "Approval Rate by Property Area:\n",
      "Property_Area\n",
      "0    0.617143\n",
      "1    0.656566\n",
      "2    0.768889\n",
      "Name: Loan_Status, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Loan_Status' to numeric: 1 for 'Y', 0 for 'N'\n",
    "df[\"Loan_Status\"] = df[\"Loan_Status\"].map({\"Y\": 1, \"N\": 0})\n",
    "\n",
    "# Re-encode categorical fields if needed (e.g., for grouping or modeling later)\n",
    "# Not strictly required here, but useful for consistency\n",
    "df[\"Gender\"] = df[\"Gender\"].map({\"Male\": 1, \"Female\": 0})\n",
    "df[\"Property_Area\"] = df[\"Property_Area\"].map({\"Rural\": 0, \"Urban\": 1, \"Semiurban\": 2})\n",
    "\n",
    "# Drop rows with any NA values in key columns\n",
    "df_clean = df.dropna(subset=[\"Loan_Status\", \"Gender\", \"Property_Area\"])\n",
    "\n",
    "# Group-wise approval rate\n",
    "approval_by_gender = df_clean.groupby(\"Gender\")[\"Loan_Status\"].mean()\n",
    "approval_by_area = df_clean.groupby(\"Property_Area\")[\"Loan_Status\"].mean()\n",
    "\n",
    "print(\"Approval Rate by Gender:\")\n",
    "print(approval_by_gender)\n",
    "\n",
    "print(\"\\nApproval Rate by Property Area:\")\n",
    "print(approval_by_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8ef96c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of applications by Gender:\n",
      "1    487\n",
      "0    111\n",
      "Name: Gender, dtype: int64\n",
      "\n",
      " Number of applications by Property Area:\n",
      "2    225\n",
      "1    198\n",
      "0    175\n",
      "Name: Property_Area, dtype: int64\n",
      "\n",
      " Number of applications by Gender + Property Area:\n",
      "1_2    171\n",
      "1_1    165\n",
      "1_0    151\n",
      "0_2     54\n",
      "0_1     33\n",
      "0_0     24\n",
      "Name: Group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Number of applications by Gender\n",
    "gender_counts = df_clean[\"Gender\"].value_counts()\n",
    "print(\"Number of applications by Gender:\")\n",
    "print(gender_counts)\n",
    "\n",
    "# Number of applications by Property Area\n",
    "area_counts = df_clean[\"Property_Area\"].value_counts()\n",
    "print(\"\\n Number of applications by Property Area:\")\n",
    "print(area_counts)\n",
    "\n",
    "# Combined group Gender + Property_Area\n",
    "df_clean[\"Group\"] = df_clean[\"Gender\"].astype(str) + \"_\" + df_clean[\"Property_Area\"].astype(str)\n",
    "group_counts = df_clean[\"Group\"].value_counts()\n",
    "print(\"\\n Number of applications by Gender + Property Area:\")\n",
    "print(group_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bf4d6e",
   "metadata": {},
   "source": [
    "Note: \n",
    "Gender encoding: Male: 1, Female: 0\n",
    "Area encoding: Rural: 0, Urban: 1, Semiurban: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a97ea18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature correlation with Loan_Status:\n",
      "Loan_Status          1.000000\n",
      "Credit_History       0.557308\n",
      "Property_Area        0.135827\n",
      "Gender               0.021239\n",
      "Dependents           0.003048\n",
      "Loan_Amount_Term    -0.017554\n",
      "ApplicantIncome     -0.025248\n",
      "LoanAmount          -0.055643\n",
      "CoapplicantIncome   -0.058194\n",
      "Name: Loan_Status, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check feature correlation with Loan_Status\n",
    "corr = df_clean.select_dtypes(include=\"number\").corr()\n",
    "print(\"Feature correlation with Loan_Status:\")\n",
    "print(corr[\"Loan_Status\"].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04edde4",
   "metadata": {},
   "source": [
    "##### Insights\n",
    "\n",
    "The data suggests **representation bias**. Male applicants make up ~81% of the dataset. Female applicants, especially in rural areas, are severely underrepresented (just 24 samples). \n",
    "\n",
    "Property area is fairly balanced, but Semiurban is slightly overrepresented.\n",
    "\n",
    "In terms of feature correclation, credit history dominates prediction. This may encode **historical bias** if not audited carefully.\n",
    "\n",
    "##### Key implications\n",
    "\n",
    "| Pipeline Stage   | Potential Bias Source                       | Mitigation Recommendation                         |\n",
    "|------------------|---------------------------------------------|---------------------------------------------------|\n",
    "| Data Collection  | Underrepresentation of women & rural groups | Stratify datasets, resample or augment data       |\n",
    "| Feature Design   | Credit history dominance                    | Audit for proxy discrimination                    |\n",
    "| Labeling         | Labels reflect historical approvals         | Assess for historical discrimination in labels    |\n",
    "| Evaluation       | Group metrics affected by small-N           | Use confidence intervals and intersectional views |\n",
    "\n",
    "\n",
    "Let's proceed with fairness metric evaluation next and incorporate these findings to inform our metric selection and interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f8dde1",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a6322b",
   "metadata": {},
   "source": [
    "#### Preprocess the dataset\n",
    "- Remove irrelevant identifiers like Loan_ID.\n",
    "- Fill in missing values using median (for numerical) and mode (for categorical).\n",
    "- Encode remaining categorical features\n",
    "- Feature scaling using StandardScaler to standardize features before feeding into logistic regression. This prevents features with large ranges (e.g., income) from dominating the model\n",
    "- Split the dataset by Loan_Status to preserve class balance in both train and test sets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dc03e439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 478, Test samples: 120\n",
      "Class distribution in y_train: [149 329]\n",
      "Class distribution in y_test: [38 82]\n"
     ]
    }
   ],
   "source": [
    "# ## Step 2: Data Cleaning and Preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Drop Loan_ID (not useful for modeling)\n",
    "df.drop('Loan_ID', axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values\n",
    "df['Dependents'].fillna(df['Dependents'].mode()[0], inplace=True)\n",
    "df['LoanAmount'].fillna(df['LoanAmount'].median(), inplace=True)\n",
    "df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0], inplace=True)\n",
    "df['Credit_History'].fillna(df['Credit_History'].mode()[0], inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['Married', 'Education', 'Self_Employed']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop('Loan_Status', axis=1)\n",
    "y = df['Loan_Status']\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split stratified by the target label\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # Maintain class balance\n",
    ")\n",
    "\n",
    "# Confirm shape and class distribution\n",
    "print(f\"Training samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}\")\n",
    "print(\"Class distribution in y_train:\", np.bincount(y_train))\n",
    "print(\"Class distribution in y_test:\", np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01931ed7",
   "metadata": {},
   "source": [
    "##### Train a baseline model using logistic regression and assess the model’s performance using accuracy, confusion matrix, and classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf6810df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8083333333333333\n",
      "\n",
      "Confusion Matrix:\n",
      " [[17 21]\n",
      " [ 2 80]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.45      0.60        38\n",
      "           1       0.79      0.98      0.87        82\n",
      "\n",
      "    accuracy                           0.81       120\n",
      "   macro avg       0.84      0.71      0.74       120\n",
      "weighted avg       0.82      0.81      0.79       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ## Step 3: Train and Evaluate Logistic Regression Model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Output results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42777d0",
   "metadata": {},
   "source": [
    "## Step 4: Fairness Metrics Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9809cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a DataFrame to analyze results\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test, columns=X.columns).reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "X_test_df[\"y_true\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f49da931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fairness by Gender:\n",
      "Gender 0 -> TPR: 1.0 | Positive Rate: 0.9 | Samples: 20\n",
      "Gender 1 -> TPR: 0.97 | Positive Rate: 0.83 | Samples: 100\n",
      "\n",
      "Fairness by Property_Area:\n",
      "Area 0 -> TPR: 0.889 | Positive Rate: 0.846 | Samples: 13\n",
      "Area 1 -> TPR: 0.975 | Positive Rate: 0.828 | Samples: 58\n",
      "Area 2 -> TPR: 1.0 | Positive Rate: 0.857 | Samples: 49\n",
      "\n",
      "Intersectional Fairness:\n",
      "Group 0_1 -> TPR: 1.0 | Positive Rate: 0.833 | Samples: 6\n",
      "Group 0_2 -> TPR: 1.0 | Positive Rate: 0.929 | Samples: 14\n",
      "Group 1_0 -> TPR: 0.889 | Positive Rate: 0.846 | Samples: 13\n",
      "Group 1_1 -> TPR: 0.971 | Positive Rate: 0.827 | Samples: 52\n",
      "Group 1_2 -> TPR: 1.0 | Positive Rate: 0.829 | Samples: 35\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Reconstruct original group info\n",
    "X_test_df = pd.DataFrame(X_test, columns=X.columns)\n",
    "X_test_df[\"Gender\"] = df.loc[X_test_df.index, \"Gender\"].values\n",
    "X_test_df[\"Property_Area\"] = df.loc[X_test_df.index, \"Property_Area\"].values\n",
    "X_test_df[\"y_true\"] = y_test\n",
    "X_test_df[\"y_pred\"] = y_pred\n",
    "\n",
    "# Define helper functions\n",
    "def compute_tpr(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        return tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    return 0\n",
    "\n",
    "def compute_positive_rate(y_pred):\n",
    "    return np.mean(y_pred)\n",
    "\n",
    "# Group-wise fairness analysis\n",
    "print(\"\\nFairness by Gender:\")\n",
    "for gender in sorted(X_test_df[\"Gender\"].unique()):\n",
    "    group = X_test_df[X_test_df[\"Gender\"] == gender]\n",
    "    tpr = compute_tpr(group[\"y_true\"], group[\"y_pred\"])\n",
    "    ppr = compute_positive_rate(group[\"y_pred\"])\n",
    "    print(f\"Gender {gender} -> TPR: {round(tpr, 3)} | Positive Rate: {round(ppr, 3)} | Samples: {len(group)}\")\n",
    "\n",
    "print(\"\\nFairness by Property_Area:\")\n",
    "for area in sorted(X_test_df[\"Property_Area\"].unique()):\n",
    "    group = X_test_df[X_test_df[\"Property_Area\"] == area]\n",
    "    tpr = compute_tpr(group[\"y_true\"], group[\"y_pred\"])\n",
    "    ppr = compute_positive_rate(group[\"y_pred\"])\n",
    "    print(f\"Area {area} -> TPR: {round(tpr, 3)} | Positive Rate: {round(ppr, 3)} | Samples: {len(group)}\")\n",
    "\n",
    "# Intersectional fairness\n",
    "X_test_df[\"Group\"] = X_test_df[\"Gender\"].astype(str) + \"_\" + X_test_df[\"Property_Area\"].astype(str)\n",
    "print(\"\\nIntersectional Fairness:\")\n",
    "for group_id in sorted(X_test_df[\"Group\"].unique()):\n",
    "    group = X_test_df[X_test_df[\"Group\"] == group_id]\n",
    "    tpr = compute_tpr(group[\"y_true\"], group[\"y_pred\"])\n",
    "    ppr = compute_positive_rate(group[\"y_pred\"])\n",
    "    print(f\"Group {group_id} -> TPR: {round(tpr, 3)} | Positive Rate: {round(ppr, 3)} | Samples: {len(group)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b23854",
   "metadata": {},
   "source": [
    "### 5. Audit Gap and recommendations\n",
    "\n",
    "After evaluating fairness metrics across gender, property area, and intersectional subgroups, several **limitations** and **gaps** remain that could obscure or distort audit conclusions.\n",
    "\n",
    "---\n",
    "\n",
    "#### Gap 1: Incomplete subgroup evaluation\n",
    "\n",
    "Some subgroups (e.g., female + rural) had **low sample counts** in the test set.\n",
    "\n",
    "- **Example**: \"Group 0_1\" has only 6 samples — resulting in potentially **unstable TPR or Positive Rate** estimates.\n",
    "- **Implication**: Fairness conclusions based on small samples may not generalize.\n",
    "\n",
    "**Next Steps**:\n",
    "- Ensure stratified test sampling retains subgroup diversity.\n",
    "- Consider bootstrapped confidence intervals or Bayesian estimation for low-n cases.\n",
    "\n",
    "---\n",
    "\n",
    "#### Gap 2: No confidence Intervals or statistical significance\n",
    "\n",
    "All fairness metrics (TPR, Positive Rate) were point estimates.\n",
    "\n",
    "- **Implication**: Without confidence intervals, we don’t know whether metric differences are **statistically meaningful** or due to noise.\n",
    "\n",
    "**Next Steps**:\n",
    "- Apply bootstrapping or use tools like `fairlearn` or `AIF360` to compute confidence intervals.\n",
    "- Use multiple testing corrections (e.g., Benjamini-Hochberg) when evaluating many metrics.\n",
    "\n",
    "---\n",
    "\n",
    "#### Gap 3: Feature-level fairness not assessed\n",
    "\n",
    "We assessed output disparities but not **feature-level sources** of potential bias.\n",
    "\n",
    "- **Examples**: Credit history, loan amount, income may have disparate impact.\n",
    "- **Implication**: Bias may be hidden in model inputs even if output metrics look balanced.\n",
    "\n",
    "**Next Steps**:\n",
    "- Perform fairness analysis for individual features.\n",
    "- Check for proxies of protected attributes or skewed distributions.\n",
    "\n",
    "---\n",
    "\n",
    "#### Gap 4: No label quality audit\n",
    "\n",
    "The label `Loan_Status` was assumed accurate and unbiased.\n",
    "\n",
    "- **Implication**: If historical decisions used to create labels were biased, the model may inherit these biases.\n",
    "\n",
    "**Next Steps**:\n",
    "- Trace label generation process for known human or systemic bias.\n",
    "- Consider re-labeling or debiasing approaches if justified.\n",
    "\n",
    "---\n",
    "\n",
    "#### Gap 5: No uncertainty or drift evaluation\n",
    "\n",
    "The audit was performed on a single snapshot of data.\n",
    "\n",
    "- **Implication**: Fairness performance might degrade over time (temporal drift).\n",
    "\n",
    "**Next Steps**:\n",
    "- Schedule recurring audits post-deployment.\n",
    "- Monitor group-wise metrics over time.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Area                   | Gap Identified                            | Recommendation                                  |\n",
    "|------------------------|-------------------------------------------|-------------------------------------------------|\n",
    "| Subgroup coverage      | Low samples for some groups               | Stratified sampling + CIs                       |\n",
    "| Metric uncertainty     | No statistical bounds                     | Bootstrap or `fairlearn` tools                  |\n",
    "| Feature inputs         | Not audited for fairness impact           | Add feature-level disparity checks              |\n",
    "| Label bias             | Not verified                              | Review or re-label based on human decision bias |\n",
    "| Generalizability       | Single data slice                         | Add recurring evaluations and drift checks      |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
